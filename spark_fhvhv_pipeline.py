"""
Minimalny pipeline Spark dla danych FHVHV (NYC, 2025-01..04)

- Wczytanie 4 plik√≥w Parquet
- Proste czyszczenie
- Metryki miesiƒôczne i godzinowe
- Zapis do r√≥≈ºnych katalog√≥w / format√≥w (Parquet, CSV, JSON)
"""

from pathlib import Path
from pyspark.sql import SparkSession, functions as F
import os

# --- ≈öCIE≈ªKI ------------------------------------------------------------------

# W Dockerze u≈ºywamy /opt/spark-data (wsp√≥lne dla wszystkich kontener√≥w)
# Lokalnie u≈ºywamy data/

DATA_DIR = Path(os.getenv("DATA_DIR", "data"))

RAW_DIR = DATA_DIR / "raw"
WH_DIR = DATA_DIR / "warehouse"
CLEANED_DIR = WH_DIR / "cleaned" / "fhvhv_2025_q1"
AGG_DIR = WH_DIR / "aggregates" / "fhvhv"
JSON_DIR = WH_DIR / "json_reports"

# --- SPARK --------------------------------------------------------------------

def create_spark():
    """
    Tworzy sesjƒô Spark po≈ÇƒÖczonƒÖ z klastrem Docker.
    Automatycznie wykrywa ≈õrodowisko i ≈ÇƒÖczy siƒô z masterem przez spark://spark-master:7077
    """
    # Sprawd≈∫ czy jeste≈õmy w kontenerze Docker
    spark_master = os.getenv("SPARK_MASTER", "spark://spark-master:7077")
    
    # Je≈õli jeste≈õmy w kontenerze Docker, u≈ºyj spark-master, w przeciwnym razie localhost
    if not os.path.exists("/.dockerenv"):
        # Lokalne uruchomienie - u≈ºyj localhost
        spark_master = "spark://localhost:7077"
        driver_host = "localhost"
    else:
        # W kontenerze Docker
        driver_host = "spark-pipeline"
    
    print(f"üîó ≈ÅƒÖczenie z Spark Master: {spark_master}")
    
    spark = (
        SparkSession.builder
        .appName("NYC FHVHV Big Data Project")
        .master(spark_master)
        .config("spark.executor.memory", "2g")
        .config("spark.executor.cores", "2")
        .config("spark.driver.memory", "1g")
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
        .config("spark.driver.host", driver_host)
        .config("spark.driver.bindAddress", "0.0.0.0")
        .config("spark.network.timeout", "800s")
        .config("spark.executor.heartbeatInterval", "60s")
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel("WARN")
    print(f"‚úÖ Po≈ÇƒÖczono z klastrem Spark!")
    return spark

# --- ETAP 1: WCZYTANIE --------------------------------------------------------

def load_fhvhv_data(spark):
    """Wczytuje 4 pliki FHVHV Parquet (2025-01..04)"""
    # months = ["01", "02", "03", "04"]
    months=["01"]
    paths = [RAW_DIR / f"fhvhv_tripdata_2025-{m}.parquet" for m in months]

    print("\nüìÇ Wczytujƒô pliki FHVHV:")
    for p in paths:
        if p.exists():
            print(f"   ‚úì {p.name}")
        else:
            print(f"   ‚ö†Ô∏è  {p.name} - nie znaleziono")

    # Filtruj tylko istniejƒÖce pliki
    existing_paths = [p for p in paths if p.exists()]
    
    if not existing_paths:
        raise FileNotFoundError("Brak plik√≥w FHVHV do wczytania!")
    
    # Konwertuj na bezwzglƒôdne ≈õcie≈ºki dla Spark
    str_paths = [str(p.absolute()) for p in existing_paths]
    df = spark.read.parquet(*str_paths)

    print(f"   ‚Üí liczba wierszy RAW: {df.count():,}")
    return df

# --- ETAP 2: CZYSZCZENIE + DODATKOWE KOLUMNY ---------------------------------

def clean_data(df):
    """
    Proste czyszczenie:
    - wyrzucamy przejazdy bez dystansu / z ujemnym dystansem
    - obcinamy absurdalne warto≈õci (np. > 200 mil)
    - wyrzucamy przejazdy z niepoprawnƒÖ bazowƒÖ op≈ÇatƒÖ
    - dodajemy kolumny czasowe + total_revenue
    """

    df_clean = (
        df
        # dystans sensowny
        .filter(F.col("trip_miles") > 0)
        .filter(F.col("trip_miles") < 200)
        # sensowna op≈Çata pasa≈ºerska
        .filter(F.col("base_passenger_fare") > 0)
    )

    # total_revenue = suma wszystkich sk≈Çadowych + napiwek
    df_clean = df_clean.withColumn(
        "total_revenue",
        F.coalesce(F.col("base_passenger_fare"), F.lit(0))
        + F.coalesce(F.col("tolls"), F.lit(0))
        + F.coalesce(F.col("bcf"), F.lit(0))
        + F.coalesce(F.col("sales_tax"), F.lit(0))
        + F.coalesce(F.col("congestion_surcharge"), F.lit(0))
        + F.coalesce(F.col("airport_fee"), F.lit(0))
        + F.coalesce(F.col("cbd_congestion_fee"), F.lit(0))
        + F.coalesce(F.col("tips"), F.lit(0))
    )

    # kolumny czasowe: miesiƒÖc, data, godzina
    df_clean = df_clean.withColumn(
        "pickup_datetime", F.to_timestamp("pickup_datetime")
    )
    df_clean = df_clean.withColumn(
        "pickup_month", F.date_format("pickup_datetime", "yyyy-MM")
    )
    df_clean = df_clean.withColumn(
        "pickup_date", F.to_date("pickup_datetime")
    )
    df_clean = df_clean.withColumn(
        "pickup_hour", F.hour("pickup_datetime")
    )

    print(f"   ‚Üí liczba wierszy po czyszczeniu: {df_clean.count():,}")
    return df_clean

# --- ETAP 3: AGREGACJE --------------------------------------------------------

def aggregate_monthly(df):
    """Metryki miesiƒôczne dla 4 miesiƒôcy"""
    monthly = (
        df.groupBy("pickup_month")
        .agg(
            F.count("*").alias("trips"),
            F.sum("total_revenue").alias("total_revenue"),
            F.avg("total_revenue").alias("avg_revenue_per_trip"),
            F.sum("trip_miles").alias("total_miles"),
            F.avg("trip_miles").alias("avg_trip_miles"),
            F.sum("tips").alias("total_tips"),
            F.avg("tips").alias("avg_tip"),
        )
        .orderBy("pickup_month")
    )
    return monthly

def aggregate_hourly(df):
    """Liczba przejazd√≥w per godzina w ka≈ºdym miesiƒÖcu"""
    hourly = (
        df.groupBy("pickup_month", "pickup_hour")
        .agg(F.count("*").alias("trips"))
        .orderBy("pickup_month", "pickup_hour")
    )
    return hourly

def aggregate_summary(df):
    """Proste globalne podsumowanie dla ca≈Çego okresu"""
    summary = df.agg(
        F.count("*").alias("total_trips"),
        F.sum("total_revenue").alias("total_revenue"),
        F.sum("trip_miles").alias("total_miles"),
        F.avg("trip_miles").alias("avg_trip_miles"),
        F.avg("total_revenue").alias("avg_revenue_per_trip"),
        F.sum("tips").alias("total_tips"),
    )
    return summary

# --- ETAP 4: ZAPIS DO PARQUET / CSV / JSON -----------------------------------

def save_outputs(df_clean, monthly, hourly, summary):
    """Zapisuje wyniki w r√≥≈ºnych lokalizacjach i formatach"""

    # katalogi
    for d in [CLEANED_DIR, AGG_DIR, JSON_DIR]:
        d.mkdir(parents=True, exist_ok=True)

    # 1) oczyszczone dane ‚Äì Parquet, partycjonowanie po miesiƒÖcu
    print(f"\nüíæ Zapis oczyszczonych danych do Parquet: {CLEANED_DIR}")
    (
        df_clean.write
        .mode("overwrite")
        .partitionBy("pickup_month")
        .option("compression", "uncompressed")
        .parquet(str(CLEANED_DIR))
    )

    # 2) metryki miesiƒôczne ‚Äì Parquet + CSV
    print(f"üíæ Zapis miesiƒôcznych metryk do: {AGG_DIR}")
    (
        monthly.write
        .mode("overwrite")
        .option("compression", "uncompressed")
        .parquet(str(AGG_DIR / "monthly_metrics_parquet"))
    )
    (
        monthly.write
        .mode("overwrite")
        .option("header", True)
        .option("compression", "uncompressed")
        .csv(str(AGG_DIR / "monthly_metrics_csv"))
    )

    # 3) wolumen godzinowy ‚Äì CSV
    (
        hourly.write
        .mode("overwrite")
        .option("header", True)
        .option("compression", "uncompressed")
        .csv(str(AGG_DIR / "hourly_volume_csv"))
    )

    # 4) globalne podsumowanie ‚Äì JSON
    import json

    summary_pd = summary.toPandas()
    summary_path = JSON_DIR / "fhvhv_summary.json"
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary_pd.to_dict("records"), f, indent=2, default=str)

    print(f"üíæ Zapisano globalne podsumowanie: {summary_path}")

# --- MAIN ---------------------------------------------------------------------

def main():
    print("=" * 80)
    print("üöÄ SPARK PIPELINE ‚Äì NYC FHVHV 2025 Q1")
    print("=" * 80)

    spark = create_spark()

    try:
        # 1. wczytanie
        df_raw = load_fhvhv_data(spark)

        # 2. czyszczenie + kolumny pomocnicze
        df_clean = clean_data(df_raw)

        # 3. agregacje
        monthly = aggregate_monthly(df_clean)
        hourly = aggregate_hourly(df_clean)
        summary = aggregate_summary(df_clean)

        # 4. zapis
        save_outputs(df_clean, monthly, hourly, summary)

        print("\n" + "=" * 80)
        print("‚úÖ Pipeline zako≈Ñczony pomy≈õlnie!")
        print("=" * 80)
        print(f"\nüìÅ Wyniki zapisane w:")
        print(f"   - {CLEANED_DIR}/ (oczyszczone dane Parquet)")
        print(f"   - {AGG_DIR}/ (agregaty Parquet + CSV)")
        print(f"   - {JSON_DIR}/ (raporty JSON)")

    except Exception as e:
        print(f"\n‚ùå B≈ÇƒÖd: {e}")
        import traceback
        traceback.print_exc()
    finally:
        spark.stop()

if __name__ == "__main__":
    main()

