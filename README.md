# System do przechowywania i analizy Big Data - NYC Taxi

Projekt studencki implementujÄ…cy dwustopniowy system analizy danych taksÃ³wek z Nowego Jorku:
- **Warstwa Big Data (Apache Spark)**: ETL, czyszczenie, agregacje, zapis do rÃ³Å¼nych formatÃ³w
- **Warstwa analityczna (Pandas)**: Wizualizacje i raporty na podstawie przetworzonych danych

## ğŸ“‹ Wymagania

- Python 3.8+
- Java JDK 8+ (wymagane przez Spark)
- Apache Spark 3.5.0+ (instalowany przez PySpark)

## ğŸš€ Instalacja

1. **Sklonuj repozytorium lub pobierz pliki projektu**

2. **UtwÃ³rz Å›rodowisko wirtualne:**
   ```bash
   python -m venv venv
   ```

3. **Aktywuj Å›rodowisko wirtualne:**
   - Windows: `venv\Scripts\activate`
   - Linux/Mac: `source venv/bin/activate`

4. **Zainstaluj zaleÅ¼noÅ›ci:**
   ```bash
   pip install -r requirements.txt
   ```

5. **Pobierz dane:**
   - UmieÅ›Ä‡ pliki Parquet z NYC TLC w katalogu `data/raw/`
   - PrzykÅ‚adowe pliki:
     - `yellow_tripdata_2025-01.parquet`
     - `green_tripdata_2025-01.parquet`
     - `fhv_tripdata_2025-01.parquet`
     - `fhvhv_tripdata_2025-01.parquet`

## ğŸ“ Struktura projektu

```
BigDataProject/
â”‚
â”œâ”€ data/
â”‚   â”œâ”€ raw/              # Oryginalne pliki Parquet z NYC TLC
â”‚   â”œâ”€ warehouse/        # Dane przetworzone przez Spark
â”‚   â”‚   â”œâ”€ cleaned/      # Oczyszczone dane (Parquet, partycjonowane)
â”‚   â”‚   â”œâ”€ aggregates/   # Agregaty (Parquet + CSV)
â”‚   â”‚   â””â”€ json_reports/ # Raporty JSON ze statystykami
â”‚   â””â”€ analysis/         # Wyniki analizy (PNG + TXT)
â”‚
â”œâ”€ spark_pipeline.py     # GÅ‚Ã³wny pipeline Spark (ETL + agregacje)
â”œâ”€ analyze_taxi_data.py  # Analiza biznesowa + wizualizacje
â”œâ”€ view_parquet.py       # NarzÄ™dzie do przeglÄ…dania plikÃ³w Parquet
â””â”€ requirements.txt      # ZaleÅ¼noÅ›ci Python
```

## ğŸ”§ UÅ¼ycie

### Krok 1: Przetwarzanie danych w Spark

Uruchom pipeline Spark, ktÃ³ry:
- Wczyta dane z `data/raw/`
- OczyÅ›ci dane (usunie bÅ‚Ä™dne wartoÅ›ci)
- Wykona agregacje (dzienny przychÃ³d, przejazdy per godzina, statystyki)
- Zapisze wyniki do rÃ³Å¼nych lokalizacji i formatÃ³w

```bash
python spark_pipeline.py
```

**Wyniki:**
- `data/warehouse/cleaned/` - oczyszczone dane Parquet (partycjonowane po dacie)
- `data/warehouse/aggregates/daily_revenue_parquet/` - dzienny przychÃ³d (Parquet)
- `data/warehouse/aggregates/daily_revenue_csv/` - dzienny przychÃ³d (CSV)
- `data/warehouse/aggregates/hourly_trips_csv/` - przejazdy per godzina (CSV)
- `data/warehouse/json_reports/distance_stats.json` - statystyki odlegÅ‚oÅ›ci (JSON)
- `data/warehouse/json_reports/financial_stats.json` - statystyki finansowe (JSON)

### Krok 2: Analiza i wizualizacje

#### Opcja A: Analiza na agregatach z Sparka (zalecane)

UÅ¼ywa przetworzonych danych z Sparka - szybsze i bardziej efektywne:

```bash
python analyze_taxi_data.py --spark
```

#### Opcja B: Analiza na surowych danych

Analizuje oryginalne pliki Parquet bezpoÅ›rednio (wymaga wiÄ™cej pamiÄ™ci):

```bash
python analyze_taxi_data.py
```

**Wyniki:**
- `data/analysis/spark_aggregates_analysis.png` - wizualizacje z agregatÃ³w Sparka
- `data/analysis/financial_analysis.png` - analiza finansowa
- `data/analysis/distance_analysis.png` - analiza odlegÅ‚oÅ›ci
- `data/analysis/time_patterns_analysis.png` - wzorce czasowe
- `data/analysis/business_analysis_report.txt` - raport tekstowy

### PrzeglÄ…danie plikÃ³w Parquet

```bash
python view_parquet.py
# lub z konkretnÄ… Å›cieÅ¼kÄ…:
python view_parquet.py "data/raw/yellow_tripdata_2025-01.parquet"
```

### Sprawdzanie Å›rodowiska

Przed uruchomieniem Spark pipeline, sprawdÅº czy wszystko jest OK:

```bash
python check_environment.py
```

Skrypt sprawdzi:
- WersjÄ™ Java (wymagana 8, 11 lub 17)
- ZmiennÄ… JAVA_HOME
- Zainstalowany PySpark
- DostÄ™pnoÅ›Ä‡ plikÃ³w danych

### Prosta wizualizacja wynikÃ³w Spark

Po uruchomieniu `spark_pipeline.py`, moÅ¼esz szybko wygenerowaÄ‡ wykresy:

```bash
python visualize_spark_results.py
```

Tworzy wizualizacje z agregatÃ³w wygenerowanych przez Spark.

## ğŸ“Š Architektura systemu

### Warstwa Big Data (Apache Spark)

**Funkcje:**
- Import duÅ¼ych zbiorÃ³w danych Parquet (miliony wierszy)
- Czyszczenie danych (filtrowanie bÅ‚Ä™dnych wartoÅ›ci)
- Agregacje (dzienne, godzinowe, statystyczne)
- Zapis do rÃ³Å¼nych formatÃ³w:
  - **Parquet** - dla dalszego przetwarzania (z partycjonowaniem)
  - **CSV** - dla Å‚atwego importu do Pandas
  - **JSON** - dla raportÃ³w i dokumentacji

**Konfiguracja Spark:**
- Tryb: `local[*]` (wykorzystuje wszystkie dostÄ™pne rdzenie)
- Adaptive Query Execution (AQE) wÅ‚Ä…czony
- Logowanie na poziomie WARN

### Warstwa analityczna (Pandas + Matplotlib)

**Funkcje:**
- Wczytywanie agregatÃ³w z Sparka (CSV, JSON)
- Analiza biznesowa (przychody, odlegÅ‚oÅ›ci, wzorce czasowe)
- Wizualizacje (wykresy liniowe, sÅ‚upkowe, boxploty)
- Generowanie raportÃ³w tekstowych

## ğŸ“ˆ PrzykÅ‚adowe analizy

### Agregacje w Spark:

1. **Dzienny przychÃ³d:**
   - Liczba przejazdÃ³w per dzieÅ„
   - CaÅ‚kowity przychÃ³d dzienny
   - Åšrednia kwota za przejazd

2. **Przejazdy per godzina:**
   - RozkÅ‚ad przejazdÃ³w w ciÄ…gu dnia
   - Identyfikacja godzin szczytu

3. **Statystyki odlegÅ‚oÅ›ci:**
   - Åšrednia, mediana, min, max
   - CaÅ‚kowita przejechana odlegÅ‚oÅ›Ä‡

4. **Statystyki finansowe:**
   - Åšrednie opÅ‚aty i napiwki
   - CaÅ‚kowity przychÃ³d
   - Liczba przejazdÃ³w z napiwkiem

## ğŸ” Informacje techniczne

### Wersje bibliotek:
- PySpark: 3.5.0+
- Pandas: 2.0.0+
- Matplotlib: 3.7.0+
- Seaborn: 0.12.0+
- PyArrow: 10.0.0+ (wymagane do odczytu Parquet)

### Wymagania systemowe:
- **PamiÄ™Ä‡ RAM:** Minimum 8GB (zalecane 16GB+ dla peÅ‚nego miesiÄ…ca danych)
- **Dysk:** ~2-5GB wolnego miejsca na przetworzone dane
- **CPU:** Wielordzeniowy procesor (Spark wykorzystuje wszystkie rdzenie)

## ğŸ“ Sprawozdanie

Projekt speÅ‚nia wymagania:
- âœ… System do przechowywania i analizy Big Data w oparciu o Apache Spark
- âœ… Import i skÅ‚adowanie w rÃ³Å¼nych lokalizacjach i formatach
- âœ… Analiza duÅ¼ych zbiorÃ³w danych (miliony wierszy)
- âœ… Agregacje i obliczenia w Spark
- âœ… Wizualizacje i raporty w Pandas

## ğŸ› RozwiÄ…zywanie problemÃ³w

### BÅ‚Ä…d: "UnsupportedOperationException: getSubject"
**Problem:** Masz zbyt nowÄ… wersjÄ™ Java (21+). Spark wymaga JDK 8, 11 lub 17 (LTS).

**RozwiÄ…zanie:**
1. SprawdÅº wersjÄ™ Java: `java -version`
2. JeÅ›li masz Java 21+, zainstaluj **JDK 17 (LTS)** z [Adoptium](https://adoptium.net/)
3. Ustaw `JAVA_HOME` na katalog z JDK 17
4. Dodaj `%JAVA_HOME%\bin` do PATH (na poczÄ…tku listy)
5. **Zamknij i otwÃ³rz nowy terminal** (zmienne Å›rodowiskowe siÄ™ odÅ›wieÅ¼Ä…)
6. SprawdÅº ponownie: `java -version` (powinno pokazaÄ‡ 17.x)

**Uruchom diagnostykÄ™:**
```bash
python check_environment.py
```

### BÅ‚Ä…d: "Java not found"
- Zainstaluj Java JDK 17 (LTS) z [Adoptium](https://adoptium.net/)
- Ustaw zmiennÄ… Å›rodowiskowÄ… `JAVA_HOME`

### BÅ‚Ä…d: "Out of memory"
- Zmniejsz rozmiar danych lub zwiÄ™ksz pamiÄ™Ä‡ dla Spark:
  ```python
  .config("spark.driver.memory", "4g")
  ```

### BÅ‚Ä…d: "File not found"
- SprawdÅº czy pliki Parquet sÄ… w `data/raw/`
- SprawdÅº czy uruchomiÅ‚eÅ› `spark_pipeline.py` przed analizÄ… agregatÃ³w

### Warningi: "winutils.exe" i "native-hadoop library"
- **MoÅ¼na zignorowaÄ‡** na Windowsie - Spark uÅ¼ywa wbudowanych klas
- Te warningi nie wpÅ‚ywajÄ… na dziaÅ‚anie pipeline

## ğŸ“š Å¹rÃ³dÅ‚a danych

Dane pochodzÄ… z [NYC Taxi & Limousine Commission (TLC)](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)

## ğŸ“„ Licencja

Zobacz plik LICENSE
